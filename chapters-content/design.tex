Based on the machine learning and deep learning applications to the task of breast cancer detection established in Chapter~\ref{ch:chapter-litsurvey}, and the datasets available for this project, design decisions specific to the deep learning pipeline to implement will be covered, along with the reasoning behind the choice of datasets to use and general considerations.

\section{Datasets Decision}

An early design decision taken as a group consisted in which datasets to use, as Chapter~\ref{ch:chapter-ethics-datasets} revealed that each dataset has different characteristics. Despite being widely used in existing literature (see Chapter~\ref{ch:chapter-litsurvey}), popular feature-based datasets containing extracted mammogram data such as the WBCD dataset \citep{Wolberg1995} will not be used as the objective of using deep learning models such as CNNs is to learn which features to extract by using the raw image in 2D space rather than data flattened into 1D arrays. If extracted features such as the ones from WBCD were used, then already successful machine learning algorithms such as SVMs or DTs could be used instead of deep learning techniques.\\

From a clinical point of view, the mini-MIAS dataset is an interesting dataset as it contains both abnormal cases and normal cases as well, resulting in three classes (normal, benign and malignant cases). Its smaller size makes it useful for initial prototyping, but has the downside of requiring more image processing techniques such as data augmentation to generate enough data to feed into the deep learning model.\\

The CBIS-DDSM dataset was chosen over the DDSM dataset as it is an updated version of the older DDSM dataset, and is curated by a trained mammographer. Additionally, it uses uncompressed images in DICOM format rather than LJPEG format, which is deprecated nowadays, resulting in much higher quality imagery. Indeed, the large uncompressed format offered by DICOM means that the mammograms can be fed into the CNN with larger sizes, allowing the model to potentially learn more low-level features. Despite the CBIS-DDSM dataset containing two different types of mammograms (calcification and masses), as well as two different views (CC and MLO), the mammograms in each case are treated as separate individual images due to the limited number of samples available.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Deep Learning Pipeline Design Analysis}

The deep learning pipeline implemented for the task of breast cancer detection can be broken down in four distinct phases condensed in Figure~\ref{fig:design-flowchart}:

\begin{itemize}
    \item \textbf{Data pre-processing}: loading a dataset in memory and processing it to gather the pair of images and labels for the classification task.
    \item \textbf{Model training}: creating a CNN model that can fit the data to learn the training set samples. Predictions are carried out once the model finished training on the validation and test sets.
    \item \textbf{Result visualisation}: The model's performance is evaluated by calculating various metrics and plotting predictions.
    \end{itemize}

\begin{figure}[ht]
\centerline{\includegraphics[width=1.1\textwidth]{figures/design/design flowchart.png}}
\caption{\label{fig:design-flowchart}A high-level flowchart of the breast cancer detection deep learning pipeline to implement.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%

\subsection{Data pre-processing}

\subsubsection{Dataset balance}

As this is a classification task, it is essential to visualise the distribution of classes in the datasets to determine whether the data balance is skewed or not (whether some classes are much more frequent than other classes \citep{Geron2019}). The class distributions are plotted in Figure~\ref{fig:design-datasets-balance}.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.94\textwidth]{figures/design/mini-mias-balance.png}
  \caption{mini-MIAS class distribution.}
  \label{fig:design-mini-mias-balance}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/design/cbis-ddsm-balance.png}
  \caption{CBIS-DDSM class distribution.}
  \label{fig:cbis-ddsm-balance}
\end{subfigure}
\caption{\label{fig:design-datasets-balance}Class distribution for the mini-MIAS and the CBIS-DDSM datasets. Histograms generated in Excel.}
\end{figure}

These bar charts reveal that the mini-MIAS dataset is heavily unbalanced as the distribution is not uniform, which must be taken into account to avoid training a biased CNN model. Potential solutions to counter this imbalance would be to either:
\begin{itemize}
    \item \textit{undersample} the dataset by dropping images altogether;
    \item \textit{oversample} the dataset, which can be achieved via data augmentation,
    \item include \textit{class weights} to give more importance to under-represented classes. 
    %https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras 
    %https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights
\end{itemize}

Undersampling the dataset can be considered as inefficient as it will diminish the number of samples the model could learn from by dropping samples from the majority class \citep{Liu2009}. As the datasets are already very small (maximum of 10,239 images in CBIS-DDSM), undersampling would be a poor strategy as it may discard useful features that could be learned. Consequently, oversampling by creating new artificial images resembling the original data is a viable solution as it was proven to increase accuracies (see Section~\ref{sec:litsurvey-data-augmentation}). Alternatively, a cheaper option in terms of computing power that does not require the dataset to be touched would be to add class weights, which will be cause the loss to become a weighted average giving more importance to less frequent classes \citep{Zhu2018}.

\subsubsection{Dataset split}

The dataset is immediately split in a training and a testing set to avoid any form of data snooping, which corresponds to the poor practice of making design decisions (either voluntary or involuntary) after having viewed the data and detecting patterns that could lead to favouring certain models or hyperparameters above others. Due to small size of the datasets used and to avoid causing further imbalance to the datasets, the splits are not randomly sampled. Instead, stratified sampling is used, which consists of maintaining representative samples from the data in both the training and the testing sets to avoid introducing sampling bias.\\

An 80\%/20\% split, often used in machine learning and seen in breast cancer detection papers \citep{Yue2018}, is used to split the mini-MIAS dataset. The CBIS-DDSM does not need to be manually divided as it was already split with the appropriate stratification when it was built \citep{Lee2017}.\\

After this step, only the training set is considered until the final results evaluations, while the testing set is set aside and forgotten about. The assumption that the data never changes during the development of the project is made, as not using the random generator with a fixed seed would cause different samples to be extracted into the training and testing sets, thus neglecting the results. 

\subsubsection{Data loading}

The mini-MIAS dataset is very small in size (339 Mb before pre-processing, 202 Mb after pre-processing), containing only 322 images. It can therefore be loaded into memory without any data loading optimisation techniques. However, the CBIS-DDSM dataset is much larger, containing 10,239 images that cover 163.6 Gb of disk space. The dataset therefore cannot be loaded in memory in a single import and needs to be loaded in batches to be fed into the CNN sequentially. % mention batches and caching

\subsubsection{Data normalisation}

Observing the pixel intensities of the images found in the datasets, it can be seen that they correspond to integers ranging from 0 to 255. However, because the weights in neural networks are small, having large input values can disrupt and slow down the training. Therefore, normalising the pixel intensities to values in the range of 0 to 1 can help fight this problem by keeping all values in a small scale.

\subsubsection{Label encoding}

As the labels for each mammogram are in categorical string format, they must be encoded into a numerical format. On the one hand, due to the sparse nature of the labels (only three categories), one-hot encoding is chosen for the mini-MIAS dataset, where a single digit may have the value 1 while the others remain at value 0 to tell apart the different labels. The one-hot encodings of the labels can be seen in Table~\ref{tab:one-hot-encoding-example}.

% The data must be fed to the neural network in binary form. One-hot encoding is therefore chosen as it suits the sparse representation of the data, which is made up of only five target categories. Only a single digit may have the value 1 in one-hot encoding, while the others remain at value 0 \cite{lec16}. The one-hot encodings of the target categories can be seen in Figure \ref{fig:one_hot_encoding}.

\input{tables/one-hot-encoding-example}

On the other hand, in the case of binary datasets like CBIS-DDSM, binary encoding can be used instead of one-hot encoding, as seen in Table~\ref{tab:binary-encoding-example}.

\input{tables/binary-encoding-example}

%%%%%%%%%%%%%%%%%%%%

\subsection{Model training}

At this stage, the training data is ready to be fed into the classifiers. The classification models will fit the transformed features from ``\textit{X\_train.csv}'' before making their own predictions, which will be compared with the real labels found in ``\textit{Y\_train.csv}'' for evaluation.\\

To complete:
\begin{itemize}
    \item Training/testing/validation split using stratified and shuffling splits (keep class balance and reorder images in directories).
    \item CNN models with pre-trained weights on ImageNet to avoid training from scratch. However, this  forces the input images to have the same size as the images in ImageNet it was trained on.
    \item Early stopping conditions.
    \item Describe process of using pre-trained weights on ImageNet, freezing this layers and training fully connected layers, then unfreezing the pre-trained model and and learning with at a slower learning rate.
\end{itemize}

Classification type: Classification on the mini-MIAS dataset will be a multi-class problem as there are three possible classes (normal, benign and malignant), whereas classification on the CBIS-DDSM dataset will be a binary problem as it only contains abnormal classes (benign and malignant).

%%%%%%%%%%%%%%%%%%%%

\subsection{Result visualisation}
\label{sec:design-results-visualisation}

These distribution of the class balance reveal that the mini-MIAS dataset is heavily unbalanced, which must be taken into account when analysing the classifiers' scores. Indeed, using an  evaluation metric such as accuracy would be misleading as it would not be representative of how well the classifier fitted the data. For instance, if a dumb classifier that always classified an image as ``normal'' was created, it would achieve 64.28\% accuracy on the mini-MIAS dataset despite  never picking up abnormal cases. Therefore, a mixture of additional metrics should be used to assess how well the model learns the mammograms data and generalises to unseen cases.

\subsubsection{F1 Score}

\textit{Precision} corresponds to the number of positive predictions for a class, whereas \textit{recall} is the number of positive instances  that are correctly predicted. Together, they can be combined into a more concise metric, which is the \textit{F1 score} (see Equation \ref{eq:f1-score}). It corresponds to the harmonic mean of precision and recall, meaning that to achieve a high F1 score, both precision and recall must be high (unlike a regular mean). Because when  the precision goes down the recall goes up, and vice versa, the F1 score is a reliable metric to evaluate a classifier since a high F1 score means a balance has been found between precision and recall \cite{Geron2019}.

\begin{equation}
\label{eq:f1-score}
    F_{1} = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} = \frac{TP}{TP+\frac{FN + FP}{2}}
\end{equation}

\subsubsection{Confusion matrix} This metric plots the number of predictions made for each class for each possible class in a table, with each row corresponding to the actual labels (the ones found in ``\textit{Y\_train.csv}'') and each column corresponding to a prediction. It is very useful for detecting which actual classes are being detected the most, and what predicted classes are misclassified as. To further highlight the misclassifications, the confusion matrix can be normalised to show a percentage \cite{Geron2019}.

\begin{itemize}
    \item Numerical metrics:
    \begin{itemize}
        \item Overall accuracy
        \item Precision
        \item Recall
        \item F1 score
    \end{itemize}
    \item Visual metrics (plots):
    \begin{itemize}
        \item Evolution of training/validation accuracies and losses over the number of epochs
        \item Confusion matrices
        \begin{itemize}
            \item Classification counts
            \item Normalised (values between 0 and 1)
        \end{itemize}
        \item Receiver Operating Characteristic (ROC) curve
    \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{General Design Decisions}

\subsection{Programming Language}

Choosing a programming language is one of the most essential aspect to take into account as it is the medium used to transform the system from design to implementation. However, choosing from over 250 different programming languages \citep{tiobe} can be tricky, which is why multiple views have to be evaluated before choosing a programming language. Four popular programming languages are considered for this project: Python, Java, R and  Javascript.\\

The first element to consider is the availability of third-party libraries used for implementing common machine learning methods, as well as data pre-processing, manipulation and visualisation techniques found in deep learning systems to avoid manually implementing them. The most popular libraries nowadays consist of Tensorflow (which is available in Python, R and Javascript) and PyTorch (which is available in Python). Additionally, Compute Unified Device Architecture (CUDA) support for GPU optimisations, CNN support and pre-trained models need to be included for the implementation of the desired deep learning breast cancer detection system, which are available in all libraries.\\

In terms of speed, compiled languages are quicker than interpreted languages. However, the main bottleneck in a deep learning system is the training phase of the model, which mainly relies on the library used rather than the language itself. Therefore, speed is not taken into account. Finally, of all the programming languages mentioned, Python is the favoured one in terms of personal preference, familiarity, and experience, especially when applied to machine learning implementations. Therefore, the wide support for machine learning libraries in Python, coupled with the personal preference for the language, make Python the obvious programming language candidate for this project.\\

For a complete review of the main pros and cons considered between when deciding between Python, Java, R and Javascript, refer to Appendix~\ref{sec:appendix-programming-languages-comparison}.

\subsection{Deep Learning Framework}

Due to the large nature of the datasets and complexity of the deep learning models to implement, powerful computing resources will be used in the form of Graphical Processing Units (GPU). A GeForce GTX 1060 6GB is provided by the School of Computer Science and remotely accessed via SSH to a lab machine equipped with the GPU in question running on CentOS.\\

To make use of the GPU's computing capabilities, deep learning frameworks with CUDA support (for parallel computing) should be used. The two most popular deep learning frameworks nowadays are Tensorflow coupled with Keras, and PyTorch. Tensorflow/Keras being relatively older than PyTorch, have got more online support, which is confirmed by the number of daily downloads Keras has compared to PyTorch (ten times more), as well as the number of mentions in academic papers (see figures  in Appendix~\ref{sec:appendix-keras_vs_pytorch})

\subsection{Interface}

A Command-Line Interface (CLI) is selected, allowing arguments and flags to be passed to execute different sections of the code. Arguments control the dataset to use, the CNN model, and the mode to run in (training or testing). Flags control the verbose mode to print more statements in the terminal for debugging purposed. The full set of instructions to run the code can be found in Appendix~\ref{ch:appendix-usage-instructions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Design Decisions Summary}

\begin{enumerate}
    \item Datasets:
    \begin{itemize}
        \item CBIS-DDSM (binary classification)
        \item mini-MIAS (multi-class classification)
    \end{itemize}
    
    \item Data pre-processing steps:
    \begin{itemize}
        \item Data augmentation
        \item Image normalisation
    \end{itemize}
    
    \item Model training:
     \begin{enumerate}
        \item todo
    \end{enumerate}
    
    \item Evaluation Metrics:
    \begin{enumerate}
        \item Numerical metrics:
        \begin{itemize}
            \item Overall accuracy
            \item F1 score
        \end{itemize}
        \item Visual metrics:
        \begin{itemize}
            \item Confusion matrices (counts and normalised)
            \item Receiver Operating Characteristic (ROC) curve
            \item Aera Under the Curve (AUC)
        \end{itemize}
    \end{enumerate}
    
    \item Programming language:
    \begin{enumerate}
        \item Python 3.7
        \item Open-source frameworks:
        \begin{itemize}
            \item Tensorflow \& Keras
            \item NumPy
            \item MatplotLib
        \end{itemize}
    \end{enumerate}
    
    \item Interface:
    \begin{itemize}
        \item Command-Line Interface
    \end{itemize}
    
\end{enumerate}
