%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Breast Cancer Detection}

\subsection{Medical imagery screening tests \& biopsies}
\label{sec:litreview-bcd-medical-imagery}

Test screenings have been used to detect early signs of breast cancer before the appearance of any symptoms (e.g. lumps that can be felt to the touch of a hand). The main methods used for breast cancer screenings are \textit{mammograms}, which are low-dosage x-rays around the breast area usually  used as initial/regular screening tests. These scans reveal backgrounds in black and dense areas in white, which may correspond to calcifications or masses (e.g. lumps or cysts). If suspicious areas are detected, mammograms are  followed by \textit{breast ultrasounds} for analysing these masses, and by \textit{breasts MRIs} (Magnetic Resonance Imaging) for detailed imagery of the breast, usually used when a malignant tumour has been detected to get more information about it, such as its size and location, or to find additional ones \citep{americanCancerSociety2019}. If any of the screenings mentioned earlier raise suspicion or reveal a potential presence of breast cancer, then biopsies can be conducted to confirm the screening tests' results. Biopsies consist of extracting cells or a small part of the breast's tissue and sending them to a lab to be analysed by pathologists to get definite results \citep{martin2019}.\\

Due to the invasive nature of biopsies, it is ideal for patients to use medical imagery tools to detect early signs of breast cancer that can be treated efficiently rather than immediately conducting a biopsy. Mammograms are the primary imagery method used for early breast cancer detection (BCD) \citep{Ramos-Pollan2012}. However, BCD using mammograms, and any form of cancer detection using medical imagery, relies on the conventional diagnoses of expert radiologists \citep{Osareh2010}. These diagnoses rest on the correct interpretation of the mammograms, which may be subject to errors due to the difficulty of correctly interpreting them \citep{Elter2009}. Indeed, mammograms are 2D images of 3D breasts that correspond to the superposition of breast tissue, which increases the difficulty for a radiologist to correctly analyse patterns as masses often naturally form due to this superposition \citep{Elter2009}.

\subsection{Early Breast Cancer Detection Systems}
\label{sec:litreview-bcd-early-cad}

To assist radiologists in their interpretations of mammograms, CAD software has been employed since the 1970s. However, pre-1990s CAD systems were very primitive and did not offer much more knowledge than the expert radiologists' knowledge. These unsophisticated ``expert'' systems consisted of manually processing and modelling pixels to construct rule-based systems that mainly used \textit{if-else-then} statements \citep{Litjens2017}, highlighting their inadequacy to learn how to recognise patterns that can be used to detect the vast possible forms that breast cancer can take.

\subsection{Towards Supervised Machine Learning-based Systems}

Towards the late 1990s, supervised machine learning techniques started replacing these expert systems, allowing hidden patterns in the mammograms' data that could not be perceived by radiologists to now be recognised by these new algorithms \citep{Litjens2017}. Machine learning-based approaches were selected over statistical approaches to replace expert systems as they were proven to be more suitable for classification tasks than traditional statistics-based approaches such as regression \citep{Paliwal2009}, especially when dealing with large, complex and high-dimensional datasets like mammogram datasets \citep{Yue2018}. This marked the shift from CAD systems that were fully designed by humans to systems that were trained on datasets of medical imagery \citep{Litjens2017}.\\

However, these machine learning models could not accurately operate on purely raw data such as the full-sized mammogram images. Indeed, all of the machine learning models tested against the task of BCD required relevant pieces of information to first be extracted from the image to solve the given task, using models such as k-Nearest Neighbour [kNN], Decision Trees [DT], Naive Bayes [NB] \citep{Asri2016}, Support Vector Machines [SVM] \citep{Ramos-Pollan2012} and Artificial Neural Networks [ANN] \citep{Yue2018}. These crucial bits of information pulled from the mammograms data correspond to features, and need to be extracted by humans before being fed to the aforementioned models for training. These features range from visual information, such as colours, edges, corners, shapes and textures \citep{Li2008}, to extracted information, such as the cell size, clump thickness, bare  nuclei, etc. \citep{Yue2018}.\\

Logically, the next step in the evolution of BCD systems is for the model to learn these features on its own directly from the data, rather than being fed hand-crafted features \citep{Yala2019}. Deep learning models, which corresponds to neural networks with hundreds of hidden layers, are based on this concept. However, these models have not been successfully implemented until recent years as they require powerful computers, usually equipped with Graphical Processing Units (GPU) to be efficiently trained. This means that until recent years, machine learning models have led the field of BCD, with some manual mammogram interpretations still being carried out by radiologists \citep{Litjens2017}, as depicted in Figure~\ref{fig:litsurvey-bcd-timeline}.

\begin{figure}[ht]
\centerline{\includegraphics[width=\textwidth]{figures/litsurvey/bcd_timeline.png}}
\caption{\label{fig:litsurvey-bcd-timeline}Timeline of the evolution of breast cancer detection (BCD) systems synthesising the information described in Sections~\ref{sec:litreview-bcd-medical-imagery}~and~\ref{sec:litreview-bcd-early-cad}.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Machine Learning Tasks \& Algorithms}
\label{sec:litreview-MLmodel-BCDapplications}

\subsection{Machine Learning Applications to Breast Cancer Detection}

\subsubsection{Types of machine learning algorithms}

Machine learning algorithms fall in different categories based on whether human supervision is required or not. The two main types of machine learning algorithms correspond to supervised and unsupervised learning. On the one hand, in supervised learning, the dataset is labelled, meaning every sample in the dataset includes a solution \citep{Caruana2006}. This label $y$ is used to make a prediction $\hat{y}$ by fitting the input features $\textbf{x}$ from a training dataset. The goal of a supervised learning algorithm is to determine the optimal parameters $\theta$ for the selected algorithm in order to minimise a loss function defined as $L(y,\hat{y})$, which corresponds to the error between $\hat{y}$ and $y$ \citep{Litjens2017}. A large variety of loss functions can be used such as the general Mean Squared Error (MSE) and Mean Absolute Error (MAE) loss functions, or more specific loss functions such as the Hinge Loss for SVMs \citep{Geron2019}. The main applications of supervised learning are classification and regression, with the former being the most relevant to BCD.\\

On the other hand, in unsupervised learning, the data is unlabelled, meaning only the input features $\textbf{x}$ are available while the labels $y$ are not \citep{Litjens2017}. This means the algorithm cannot optimise its hyperparameters, which correspond its configurations used to define it before training \citep{Bergstra2013}, by minimising a loss function. Instead, the algorithm needs to automatically create clusters in the dataset in order to separate them into different groups. The main applications of unsupervised learning are clustering, anomaly detection, data visualisation and dimensionality reduction \citep{Geron2019}, rendering them irrelevant to breast cancer detection. Two other categories of machine learning algorithms exist, corresponding to semi-supervised learning and reinforcement learning, but are also irrelevant to the task of detecting breast cancer.\\

Among the two types of machine learning algorithms, the most pertinent one for the task of BCD is supervised learning as datasets of mammograms need to contain properly labelled data for each sample, indicating the status of the mammogram, i.e. no tumour, benign tumour, malignant tumour \citep{Shen2017}.

\subsubsection{Types of machine learning tasks}

Two types of machine learning tasks are relevant to medical imagery analysis, including mammogram analysis for BCD: \textit{detection} (classification) and \textit{segmentation} \citep{Litjens2017}. 

\paragraph{Detection} corresponds to the classification of a medical image or exam, which is an interpretation that used to be entirely carried out by a radiologist before the appearance of CAD systems. The classification can be either binary or multi-class, depending on the data used (see Section~\ref{sec:design-dataset-decision}). With datasets like the ``Curated Breast Imaging Subset of DDSM'' \citep{Lee2017}, the classification is binary as mammograms can only be classified as either ``benign'' or ``malignant''. However, when using datasets like the ``Digital Database for Screening Mammography'' \citep{DDSMdataset2001}, the classification becomes more interesting from a clinical point of  view as mammograms can either be classified as ``normal'', ``benign'' or ``malignant'' \citep{Litjens2017}. An example of classification between benign and malignant mammograms can be found in Figure~\ref{fig:classification_example}, reinforcing the complexity that comes with interpreting mammograms for radiologists and the need for accurate and reliable CAD systems to improve the prediction accuracies.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/litsurvey/classification_benign.png}
  \caption{A benign mammogram.}
  \label{fig:classification_benign}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/litsurvey/classification_malignant.png}
  \caption{A malignant mammogram.}
  \label{fig:classification_malignant}
\end{subfigure}
\caption{\label{fig:classification_example}Example of a breast mammogram classification, showing benign (left) and malignant (right) mammograms. Images retrieved from the mini-MIAS dataset (Suckling, 1994).}
\end{figure}

\paragraph{Segmentation} corresponds to the classification of each pixel in the image based on the class the pixels belongs to, without distinguishing pixels from the same class. All objects in the image belonging to the same class will be classified in the same grouping of pixels \citep{Geron2019}. In breast cancer detection, segmentation can be used to highlight masses such as calcifications, cysts or fibroadenomas \citep{breastcancerorg2018} in mammograms by separating the masses from the background. An example is shown in Figure~\ref{fig:segmentation_example}, where mammograms are partitioned into non-overlapping segments, revealing potential masses in the image \citep{Punitha2018}.\\

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.52\textwidth]{figures/litsurvey/segmentation_example_original.jpg}
  \caption{Original mammogram images.}
  \label{fig:original-mammogram}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.52\textwidth]{figures/litsurvey/segmentation_example_segmented.jpg}
  \caption{Segmented mammogram images.}
  \label{fig:segmented-mammogram}
\end{subfigure}
\caption{\label{fig:segmentation_example}Example of a breast mammogram segmentation, showing the original mammogram (left) and the segmented image (right), depicting large masses. Images retrieved from Punithaet al. (2018).}
\end{figure}

Other machine learning tasks that have been used in medical imagery analysis consist of content-based image retrieval (retrieving similar images from a database) or image enhancement (erasing obstructing elements from an image and increasing quality) \citep{Litjens2017}, but will not be further explored as they are not directly relevant to the task of breast cancer detection.\\

The task of classification (detection) can be used to interpret whether a breast is affected by cancer through the analysis of databases of mammograms, while the task of segmentation can be used to localise a tumour within a breast by finding regions that may correspond to masses, pinpointing potential danger areas that may lead to cancerous cells.

\subsection{Comparison of BCD Supervised Learning Algorithms}

Since the late 1990s, a rich array of supervised machine learning algorithms have been applied and tested against the task of BCD, contributing to improving accuracies for detecting breast cancer \citep{Yue2018}. The main types of algorithms used in BCD, which consist of k-Nearest Neighbour (kNN), Naive Bayes (NB), Support Vector Machines (SVM), Decision Trees (DT) and Artificial Neural Networks (ANN), are briefly explained in the ensuing sections from most simple to most complex. Their performances in BCD are then compared to draw a picture of the advantages and disadvantages that each method brings.

\subsubsection{k-Nearest Neighbours}
\label{sec:litreview-knn}

k-Nearest Neighbours (kNN) is often used as an initial benchmark when studying a dataset with no prior knowledge \citep{peterson2009k}. It is a non-parametric and lazy model, as it does not learn the data's pattern but rather classifies a test sample by looking at its $k$ nearest neighbours \citep{Yue2018}. The sample data point's nearest neighbours are determined by using distance metrics such as the Euclidian distance, which is the most widely used metric \citep{peterson2009k}. Equation~\ref{eq:euclidian-distance} (adapted from \cite{russell2002artificial}) calculates the distance between two data points $s$ and $p$ in $n$-dimensional space. An example of how a kNN classifier would be used to distinguish between a benign and a malignant tumour using $k=3$ is depicted in Figure~\ref{fig:litsurvey-knn-example}.

\begin{equation}
\label{eq:euclidian-distance}
    d(s,p)=\sqrt{\sum_{i=1}^{n}(s_i-p_i)^2}
\end{equation}

\begin{figure}[h]
\centerline{\includegraphics[width=0.4\textwidth]{figures/litsurvey/knn.png}}
\caption{\label{fig:litsurvey-knn-example}Example of a kNN classifier distinguishing between benign (blue square) and malignant (red triangle) tumours for a test data sample (green circle) using $k=3$. The test sample is classified as malignant as there are two red triangles and one blue square amongst the three neighbours. Figure retrieved from T. Srivastava (\url{https://tinyurl.com/y3jqco49}).}
\end{figure}

kNN has been tested on datasets of mammograms such as the ``Wisconsin Breast Cancer Wisconsin'' (WBCD) dataset, which contains ten extracted features such as clump thickness, cell size/shape, etc. \citep{Wolberg1995}. kNNs calibrated with $k=1$ and using the Euclidian distance achieved 98.25\% accuracy \citep{Sarkar2000} and 98.70\% accuracy \citep{AhmedMedjahed2013} on the WBCD dataset when compared with other values of $k$ ranging from 1 to 15 and using other distance metrics such Cityblock distance, cosine distance and correlation.  Despite Sarkar et al. and Medjahed et al. reporting that a value of $k=1$ seemed to yield the most accuracy, these 1-NN classifiers do not actually learn the data's patterns and often underperform compared to other classifiers mentioned below, especially compared to SVMs and ANNs which can surpass accuracies of 99\% \citep{Yue2018, Asri2016, Montazeri2016}. Nevertheless, the results achieved by kNN remain fruitful when used as a primary benchmark before testing more advanced models, such as the ones described below.

\subsubsection{Naive Bayes}

Naive Bayes uses Bayes' theorem and the assumption that all input features are independent from one another, which can be described as the input features $\textbf{x}=(\textbf{x}_1, ..., \textbf{x}_n)$ being independent given a class label $C$ in Equation~\ref{eq:naive-bayes} \citep{rish2001empirical}.

\begin{equation}
\label{eq:naive-bayes}
    P(\textbf{x}|C)=\prod_{i=1}^{n}P(\textbf{x}_i|C)
\end{equation}

This assumption leads to a naive model that despite not learning the data's underlying pattern (in a similar fashion to kNN), still offers competitive results in practice \citep{russell2002artificial}, notably in the field of medical imagery analysis \citep{rish2001empirical}. Naive Bayes achieves an accuracy of 93\% on the WBCD dataset \citep{Kharya2014}, which is comparable to the aforementioned 1-NN classifiers as it does not learn the data's patterns and use extracted features rather than the original mammograms. NB still remains useful for assessing benchmark classification to compare with more advanced models.

\subsubsection{Decision Trees}
\label{sec:litsurvey-dts}

Unlike kNN and NB, the decision tree algorithm is a simple yet powerful one for fitting data. It works like a flowchart mapping samples' input feature vectors attributes and values, creating a tree made up of different types of nodes. Each non-leaf nodes tests one of the feature vectors attributes, branching out to a deeper node based on the attribute's value. Once a leaf node is reached after multiple tests, a classification decision is made \citep{quinlan2014c4}. An example of a decision tree applied to breast cancer detection can be found in Figure~\ref{fig:litsurvey-dt-example}, illustrating how attributes and their values are used to classify a tumour as benign or malignant.\\

\begin{figure}[ht]
\centerline{\includegraphics[width=0.65\textwidth]{figures/litsurvey/dt.png}}
\caption{\label{fig:litsurvey-dt-example}Example of a decision tree classifier distinguishing between benign and malignant tumours based on three extracted features from a dataset of mammograms: the size of the bare nuclei, the thickness of the clump and the uniformity of the cell size. Figure created by Yue et al. (2018).}
\end{figure}

The most popular implementations of decision trees use entropy-based impurity metrics to generate the tree, such as the ID3, C4.5 and C5.0 decision tree algorithms \citep{Yue2018}. A node reaches an entropy value of 0 when it is ``pure'', i.e. it contains only instances of one class (either only benign or only malignant data samples).\\

The extra complexity offered by decision trees such as the C4.5 classifier does do not offer improved results compared to kNN and NB classifiers, with 94.56\% achieved using J48 (Java implementation of C4.5) on the WBCD dataset \citep{Sumbaly2014}, which is still falling short of the performance achieved by SVMs and ANNs that exceed 99\% \citep{Yue2018}. Nevertheless, inserting decision trees into hybrid systems by combining them with other machine learning algorithms such as SVMs and NBs increases the accuracy to 97.13\% on the WBCD dataset \citep{Kumar2017}, which is closer to those achieved by SVMs and ANNs \citep{Yue2018}. Indeed, these hybrid systems use the advantages of each method, cancelling out the negatives of each, but come at the cost of being complicated to engineer and still requiring hand-crafted features to be fed to them as input, which is confirmed by their decision of testing these algorithms on the WBCD dataset rather than datasets containing raw images like DDSM.

\subsubsection{Support Vector Machines}
\label{sec:litsurvey-svms}

An SVM consists of a \textit{maximal margin classifier}, which aims to find the hyperplane that separates two classes the most, and the \textit{kernel trick}, used to separate non-linear data. In Figure~\ref{fig:litsurvey-svm-example}, a visual example of a maximum margin hyperplane for linearly separating benign and malignant tumours is shown.\\

\begin{figure}[ht]
\centerline{\includegraphics[width=0.6\textwidth]{figures/litsurvey/svm.png}}
\caption{\label{fig:litsurvey-svm-example}Example of a SVM classifier's maximum margin hyperplane found to separate benign and malignant tumours based on two extracted features from a dataset of mammograms: the size of the bare nuclei and the uniformity of the cell size. Figure created by Yue et al. (2018).}
\end{figure}

In the case of non-linear data, the training data is mapped to a higher dimension where they can be linearly separated. This is achieved by using the kernel trick, which maps the input feature vector to a higher dimension by using the dot product, but does not carry out the transformation, which would exponentially increase the size of the feature space and consequently the training time \citep{Geron2019}. This kernel trick is what allowed SVMs to become one of the most widely used machine learning models in many fields, including medical imagery analysis, up until today \citep{Yue2018}.\\

Many kernels can be chosen for SVM classification, ranging from Polynomial kernel for image processing to Radial Basis Function (RBF) to Gaussian kernels for general tasks when there is no prior knowledge of the data, heavily influencing its performance \citep{amari1999improving}. For the task of breast cancer detection, RBF outperformed Polynomial kernels on two small datasets\footnote{``\textit{Fine needle aspirate of breast lesions}'' (FNAB) dataset and ``\textit{gene microarrays}'' dataset \citep{Osareh2010}.} containing extracted features, with the RBF kernel managing 98.80\% and 96.33\% accuracies on both datasets while Polynomial reached 97.09\% and 95\% accuracies \citep{Osareh2010}, depicting why RBF is often chosen over other kernels. Compared to the previously mentioned ML methods, SVMs seem to learn the extracted features more efficiently, reaching 97.13\% accuracy on the WBCD, while kNN attained 95.27\%, NB 95.99\% and DT 95.13\% \citep{Asri2016}. \\

When applied to datasets containing raw images such as CBIS-DDSM, SVMs coupled with feature extraction techniques such as grey-level co-occurrence matrices (GLCM) achieved 63.03\% accuracy \citep{Sarosa2018}; while on smaller datasets like  mini-MIAS, SVMs coupled with texture-based features extracted from pre-processed images resulted in 92\% accuracy \citep{Vishrutha2014}. An accuracy of 97.2\% was achieved using the simplest form of SVMs on the WBCD dataset \citep{Bennett1998}, while 99.51\% accuracy was reached with SVMs on the same dataset when selecting the five best features \citep{Akay2009}, clearly characterising the importance of feature selection and image processing techniques for ML algorithms such as SVMs.

\subsubsection{Artificial Neural Networks}
\label{sec:litsurvey-anns}

ANNs form the basis of contemporary deep learning and are at the heart of the techniques mentioned in Section~\ref{sec:litsurvey-DLtechniques-CNN}. Originally inspired by the neuron connections found in the human brain \citep{mcculloch1943logical}, ANNs correspond to a collection of neurons (units) that ``fire'' an output if the linear sum of its weighted inputs tops a threshold. These neurons are placed in hierarchical layers (input, hidden and output layers) which are connected via weighted links, leading to a \textit{fully connected} neural network when all the neurons from one layer communicate with all the neurons in the following layer. Input layer neurons accept the feature vectors $\textbf{x}$, hidden layer neurons process the data and output layer neurons represent an outcome for the classification \citep{russell2002artificial}. Each neuron's output is determined by its activation function, which is usually a non-linear function like step, sigmoid, Tanh or ReLU (chaining linear functions results in a linear function) \citep{Litjens2017}. Figure~\ref{fig:litsurvey-ann-example} depicts an example fully-connected ANN used to classify benign and malignant tumours using six input neurons, eight hidden neurons and one output neuron.\\

\begin{figure}[ht]
\centerline{\includegraphics[width=\textwidth]{figures/litsurvey/ann.png}}
\caption{\label{fig:litsurvey-ann-example}Example of an ANN classifier distinguishing between benign and malignant tumours based on six extracted features from a dataset of mammograms. Figure created by Yue et al. (2018).}
\end{figure}

Like the previous algorithms, neural networks learn by minimising the loss $L$ between the prediction $\hat{y}$ and the labels $y$. This is done via the backpropagation algorithm (BP), which propagates the error from the output layer back to the hidden layers for each sample, allowing the links' weights to be adjusted accordingly to minimise that error \citep{russell2002artificial}. A cycle of all the training samples is  referred to as an epoch. The most common way to do this is by using a loss function with Stochastic Gradient Descent (SGD) \citep{Litjens2017}. This learning ability found in ANNs is what gives them their potential, but also their complexity due to the large number of hyperparameters used to fine-tune them and their capacity to overfit the data when over-engineered. Indeed, combinations of neural networks hyperparameters may involve the network's structure (number of layers and neurons in each layer), the learning rate and momentum for SGD, the regularisation techniques and parameters, the activation functions and the stopping conditions to name a few \citep{sklearn-MLP-2019}.\\
 
Shallow ANNs, also referred to as Multi-Layer Perceptrons (MLP), immediately showed promising results when first applied to breast cancer detection, reaching 95.2\% accuracy with only a basic 3-layer architecture using BP \citep{Wu1993}, which already surpassed experts' predictions by 3-5\% accuracy \citep{Yue2018}. However, due to computational constraints, deeper ANNs containing more hidden layers could not be efficiently utilised until recent years \citep{Litjens2017}. Instead, innovative variants to basic ANNs have been used to reduce training times, deal with multi-class classification with more ease, and yield predictions  with higher accuracies. Some of these ANN variants include networks such as Probabilistic Neural Networks (PNN) that replace the standard sigmoid activation function with an exponential function to find the training sample that is closest to the testing sample, achieving 97.23\% and 93.39\% accuracies on the two small datasets of extracted features (mentioned in Section~\ref{sec:litsurvey-svms}), which are larger than those achieved by kNN and Polynomial SVMs \citep{Osareh2010}. Another approach consisted of combining ANNs in hybrid systems (similar to those mentioned in Section~\ref{sec:litsurvey-dts}). When combined to Association Rule learning (AR) for reducing the number of features in the WBCD dataset from 9 to 5, ANNs and AR reached 95.6\% accuracy, which is extremely similar to the basic ANN on its own, but in almost half the epochs, converging in 33 epochs rather than 61 epochs, thus considerably reducing training time \citep{Karabatak2009}. Following that mindset of reducing the complexity of the WBCD dataset's features, Genetically Optimised ANNs (GOANN) use genetic programming (an ML technique that evolves towards a solution based on Darwin's Theory of Evolution) to determine the best features to use from the WBCD dataset and to optimise the network's weights and structure, producing an impressive accuracy of 99.26\% \citep{Bhardwaj2015}.

\subsubsection{Supervised machine learning algorithms comparison}

The five previously explored supervised machine learning algorithms all have one commonality: they heavily rely on the quality of the features extracted from the mammogram images as input to gain performance, and they cannot make use of the raw mammogram image in 2D space as input. This is confirmed by the datasets used for the task of BCD, as only datasets containing extracted features such as WBCD were used.\\

On their own, each algorithm showed limitations that prevent it from performing well. However, combined to form hybrid systems (e.g. DT + SVM + NB or ANN + AR), their performance increased in the form of higher accuracies or shorter training times, but so did their complexity to tune. It is worth noting that the slight differences when using identical algorithms on the same datasets can be due to the diverse training strategies involving unique training/testing/validation splits, image pre-processing steps and number of folds in cross-validation \citep{Yue2018}.\\

The next step for these supervised learning algorithms is to move away from feature extraction and redirect the effort towards new models that can automatically extract features from images rather than optimising and fine-tuning the hyperparameters and training strategies of existing algorithms. The most efficient way nowadays to achieve this is through Convolutional Neural Networks (CNN), which ingest the raw mammogram images in 2D space rather than extracted features or flattened images (transformed from 2D to 1D) where all spatial information is lost.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CNNs \& Deep Learning techniques}
\label{sec:litsurvey-DLtechniques-CNN}

\subsection{Convolution Neural Networks}

\subsubsection{Motivation for CNNs over traditional neural networks}

CNNs are a type of neural network inspired by the human visual cortex, where neurons have local receptive fields that only react to visual stimuli originating from a region of the visual field. The combination of all receptive fields covered by overlapping neurons forms the whole visual field \citep{Li2008}. This architecture makes them very efficient at performing complex visual tasks, marked by the first milestone for CNNs with the LeNet-5 architecture trained to recognise handwritten bank cheque digits \citep{LeCun1998}.\\

CNNs differ from traditional ``shallow'' ANNs as they are not fully connected. Indeed, CNNs are partially connected, with neurons in one layer only connected to a few neurons from the previous layer, meaning that they can work with large images \citep{Geron2019}. CNNs nowadays can work with images thousands of pixel large, including the ones found in mammogram datasets described in Chapter~\ref{ch:chapter-ethics-datasets}, processing them much faster than traditional machine learning methods. In a fully-connected neural network with only 100 neurons in the first layer, a 1,000 x 1,000-pixel image would already have 100,000,000 connections\footnote{$1000 \cdot 1,000 = 1,000,000$ px; $1,000,000$px $\cdot 100$ neurons $= 100,000,000$ connections.} in that first layer alone.

\subsubsection{CNN structure}

The structure of CNNs builds on top of the concepts of traditional ANNs by piling stacks of convolutional layers and pooling layers that are followed by a shallow ANN for classification (see Figure~\ref{fig:litsurvey-CNN-example}). The goal of the convolutional and pooling layers is to reduce the input images into a form that is simple enough to be processed by the fully connected layers, retaining only useful information from the original image \citep{Shen2017}.

\begin{figure}[ht]
\centerline{\includegraphics[width=\textwidth]{figures/litsurvey/CNN example.png}}
\caption{\label{fig:litsurvey-CNN-example}Example of a typical CNN adapted for multi-class breast cancer detection. Figure adapted from S. Saha (\url{https://tinyurl.com/y9mmosuq}).}
\end{figure}

\paragraph{Convolution Layers}

Neurons in the first convolutional layers are only connected to pixels in their receptive fields and not connected to every pixel in the image. Similarly, neurons in deeper convolutional layers are only connected to neurons in a small zone from the previous layer. This allows CNNs to first focus on low-level features, which are progressively assembled into high-level features as they get deeper. The more the receptive fields are spaced out (referred to as the stride), the smaller the next layer will be, thus considerably reducing the complexity of the CNN \citep{Shen2017}. Convolution is the mathematical operation that slides a moving a filter $f$ over an image $I$ to calculate a weighted sum (see Equation~\ref{eq:2d-convolution}, \cite{szeliski2010computer}). This 2D operation is possible in CNNs as layers are represented in 2D space and do not need to be flattened into a 1D array like with traditional neural networks, thus preserving the spatial information of images \citep{szeliski2010computer}.

\begin{equation}
\label{eq:2d-convolution}
    \hat{I}(x,y)=(I*f)(x,y)=\sum_{k}\sum_{l}I(k,l)\cdot f(x-k, y-l)
\end{equation}

The weights of the neurons in convolutional layers correspond to the filters, which are learned during the training phase by using optimisation techniques such as gradient descent. These filters allow a layer of neurons to highlight the areas of the image that activate the filter the most. As each layer has multiple filters, different features can be simultaneously detected in the layer's input. The result of each filter on a convolutional layer's input (the output of the previous layer) corresponds to a feature map. These multiple feature maps are all stacked together to form the convolutional layer's output.

\paragraph{Pooling Layers}

Pooling layers are similar to convolutional layers as neurons are only connected to neurons from a small region in the previous layer. The difference is that this layer is not trainable as its neurons have no weights. Indeed, it is only used to downsample the image as it traverses the network to diminish the load on the GPU. It does so by calculating an aggregate of its inputs based on a function, which can either be a maximum or an average function (see Figure~\ref{fig:litsurvey-max-vs-avg-pooling}). \textit{Maximum pooling} returns the maximum value of the covered portion of the image, whereas \textit{average pooling} returns the average of all values. Maximum pooling is the preferred option in CNNs as it retains the most robust features only and acts as a noise suppressant by discarding noisy activations \citep{Krizhevsky2012}.\\

\begin{figure}[ht]
\centerline{\includegraphics[width=\textwidth]{figures/litsurvey/max-vs-avg-pooling.png}}
\caption{\label{fig:litsurvey-max-vs-avg-pooling}Difference between max pooling and average pooling using a 2x2 window and stride 2 (left) to downsample an image (right). Figure adapted from W. Ong (\url{https://tinyurl.com/y25cke6l}).}
\end{figure}

Other benefits of pooling layers are the lower memory usage and the number of trainable parameters linked to smaller images, and especially the invariance it provides to the CNN as it will not break down when fed images that contain features of different sizes than the ones seen in the training dataset \citep{Shen2017}.

\paragraph{Fully connected layers \& Activation functions}

Similarly to ANNs, activation functions are used to connect the convolutional and pooling layers. The most common activation function used in CNNs is the Rectified Linear Unit (ReLU).\\

At the end of the stack of convolutional and pooling layers, a fully connected MLP is placed. This dense neural network takes the flattened output of the stacked convolutional and pooling layers (which are transformed from 2D to 1D) and performs the classification tasks by using the features learned by the convolutional layers in a condensed format. Depending on the number of classes to predict, a softmax activation can be used for multi-class classification or a sigmoid for binary classification.

\subsubsection{CNN Architectures}

Deep CNN models such as AlexNet and VGG, which have won the \textit{ImageNet Large Scale Visual Recognition Challenge}\footnote{ImageNet challenge is a competition used to measure the performance of CNNs: \url{http://image-net.org/}.} in 2012 and 2014, remain very popular nowadays, especially in domains like medical imagery analysis where accuracies of 97.9\% have been reached in BCD using VGG16 \citep{Wang2016}.\\

AlexNet is a CNN made up of five convolution layers that differs from traditional CNNs as not all convolution layers are separated by pooling layers \citep{Krizhevsky2012}. VGG architectures followed  this concept by stacking multiple convolution layers with smaller filter sizes to pick up more complex features. Multiple variants of VGG exist based on the number of layers (VGG16 has sixteen layers and VGG19 has nineteen layers), but due to its depth, these takes a long  time to train  and  run the risk of vanishing gradients \citep{simonyan2014very}. These are caused by the backpropagration algorithm coupled with gradient descent that lead to an exponentially smaller gradient while going  back up the initial layers, which eventually prevents the network from learning as the weights and biases are no longer updated \citep{russell2002artificial}. This problem does not occur in shallow ANNs.\\

More complex architectures were created subsequently to avoid creating deeper networks, such as ResNet, which uses residual modules, GoogLeNet, which uses inception modules, and MobileNet, which aims at maximising accuracy with little computing resources available; but are not covered in this context survey as efficiency gains is not an important factor in medical data, only the accuracy of the predictions matters \citep{Litjens2017}.
% https://analyticsindiamag.com/mobilenet-vs-resnet50-two-cnn-transfer-learning-light-frameworks/


\subsection{Deep Learning Applications in Breast Cancer Detection}

\subsubsection{Main challenges}

Implementing deep learning models requires lots of data to achieve acceptable performance levels. However, labelled datasets are not always abundantly available as they require large amounts of time and computing resources to engineer large datasets with millions of images \citep{Krizhevsky2012} like ImageNet, which contains over 15 million high definition images from 15,000 different classes \citep{Deng2010}. With databases of mammograms barely exceeding 10,000 images, one of the challenges of implementing a deep learning CAD system is to gain access to the computing resources needed to process the data and implement the model, as well as overcoming the problem of small amounts of data while avoiding overfitting it. Overfitting occurs when the model learns the data too well (e.g. detail and noise) and does not generalise well to new cases as it only recognises cases it has seen \citep{dietterich1995overfitting}.

\subsubsection{Transfer learning}
\label{sec:litreview-transfer-learning}

A commonly used deep learning technique when only a little amount of data is available is transfer learning, which makes use of CNN models pre-trained on large general datasets. The knowledge gathered by high-performing CNNs in other general domains that have larger datasets can be transferred to a related domain such as medical imagery \citep{Falconi2019}.\\

These models are designed to classify millions of images across thousands of different classes and can easily be adapted to any classification task by replacing the dense output layer that makes the actual prediction with a layer containing one neuron per class to predict. Falconi et al. demonstrated how transfer learning from general domain datasets such as ImageNet could be transferred to the domain of mammograms using datasets  such as CBIS-DDSM, reaching accuracies of 78.4\% with the ResNet-50 model \citep{Falconi2019}.\\

Shen et al. showed how using common CNN architectures such as VGG or ResNet pre-trained on ImageNet resulted in accuracy increases. For instance, the accuracy improves by 2-27\% based on the number of patches used \citep{Shen2017}, while Diaz et al. demonstrated how two ResNet-50 models were tested with different weight initialisation: one on ImageNet weights and the other with random weights using the CBIS-DDSM dataset. The model using transfer learning achieved  84\% accuracy compared to 75\% accuracy \citep{Diaz2018}, clearly depicting the advantage of using CNNs with pre-trained weights. 

\subsubsection{Regularisation techniques}

The drawback of the power showcased by deep neural networks such as CNNs is their tendency to overfit the data. To this end, new regularisation techniques were introduced to prevent overfitting.

\begin{figure}[ht]
\centerline{\includegraphics[width=\textwidth]{figures/litsurvey/Data augmentation examples.png}}
\caption{\label{fig:litsurvey-Data augmentation example}Example of transformations applied to a mammogram to generate new images. Original image retrieved from the mini-MIAS dataset \citep{Suckling1994}.}
\end{figure}

\paragraph{Data augmentation}
\label{sec:litsurvey-data-augmentation}

Another technique to counter small datasets is data augmentation, often used when attempting to learn a small dataset using complex deep learning models that have a large number of parameters, which may naturally lead to overfitting \citep{Jadoon2017}. The data is ``augmented'' by artificially creating similar realistic variants of the images found in the training set, considerably increasing the training set size. A varying amount of transformations can be applied to each image such as translation, rotation, scaling, shear, horizontal/vertical flips, brightness and contrast increases (see Figure~\ref{fig:litsurvey-Data augmentation example}) \citep{Hepsag2017}. Chen et al. show how applying data augmentation using affine transformations increases the accuracy from 83.6\% to 88.14\% using a ResNet-50 model \citep{Chen2019}.

\paragraph{Dropout}

Simply implementing dropout, the most popular regularisation technique for deep neural networks, in any CNN has been proven to boost the accuracy by 1 to 2\% \citep{Geron2019} at the cost of training time, even for state-of-the-art neural networks tested on large datasets like ImageNet \citep{Srivastava2014}. Despite training times being increased by 2 to 3 times, the gains in accuracy compensate for the extra time required to train the models implementing dropout.\\

Dropout works by randomly ignoring neurons in any layer (except the output layer) during the forward and backward passes of training, including its input and output connection weights, which helps prevents neurons from co-adapting too much with their neighbours and overfitting the data as they now have to be useful on their own. Essentially, new thinner networks are created at each training step (see Figure~\ref{fig:litsurvey-dropout}), and identical networks will never be sampled in the same training phases as there are $2^N$ possible networks, where $N$ is the number of dropable neurons. The number of neurons dropped in a layer is controlled by the dropout rate hyperparameter $p$, dictating the probability of a neuron being dropped. During testing, the neurons are no longer dropped, and an averaged ensemble of all the thinner trained networks is used. This leads to models that generalise better thanks to neurons that are less sensitive to noise and small changes in the input \citep{Srivastava2014}.

\begin{figure}[ht]
\centerline{\includegraphics[width=\textwidth]{figures/litsurvey/dropout.png}}
\caption{\label{fig:litsurvey-dropout}Example of standard neural network (left) and a neural network with dropout applied (right). Figure retrieved from Srivastava et al. (2014).}
\end{figure}

\subsubsection{Technological advances}

The main contributor to the rise of deep learning is linked to the large scale spread and availability of  Graphical Processing Units (GPU) in recent years. GPUs are much more powerful than the general-purpose Central Processing Units (CPU), as they can process large amounts of data in parallel. Initially designed for computer graphics in video games, GPUs are extremely efficient for large-matrix operations, rendering them vital in the field of deep learning \citep{Caulfield2009}. GPU-computing libraries such as CUDA or OpenCL make it possible to use the processing power of GPUs, leading to computing times that are 10 to 30 times faster than CPUs \citep{Litjens2017}.\\

However, GPUs are complicated to use efficiently from a low-level perspective, which is why the collection of open-source software libraries available online has been essential for the rise of deep learning. These libraries allow high-level GPU-efficient implementations of the most important deep learning methods and operations, allowing focus to be placed on efficiently implementing deep learning pipelines rather than low-level GPU optimisations \citep{Litjens2017}. The most popular packages nowadays include Tensorflow \citep{tensorflow2015-whitepaper} coupled with Keras \citep{chollet2015keras} and PyTorch \citep{pytorch}.\\

Implementing CNNs entails another drawback linked to the memory requirements (RAM) and the use of GPUs. Indeed, input data, neuron weights and biases all need to be stored in memory as the data propagates through the neural network, especially during training as the data needs to be retained during backpropagation to calculate the error gradients \citep{Geron2019}. For GPU optimisation, this data is formatted as dense vectors for parallel processing optimisations, which can increase local RAM requirements to over 7.5GB for a CNN like ResNet-50 \citep{Hanlon2016}.\\

Without the rise of popularity in both GPUs and software libraries to take advantage of the additional processing power offered by GPUs, and the increase in other computing resources such as the amount of GPU and RAM, deep learning would not have been successfully applied to fields such as medical imagery analysis nowadays.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUMMARY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}
\label{sec:litsurvey-summary}

CAD systems have been developed since the 1970s to assist radiologists in their interpretations of mammograms, starting with primitive expert systems. These systems were replaced by supervised machine learning algorithms but required hand-crafted features to be extracted from the images. The performance of the algorithms heavily relied on the quality of the features, which could not operate on raw mammograms. Therefore, deep learning algorithms have gained traction recently, notably CNNs, to automatically learn which features to extract from the images, thus preserving their 2D spatial property. However, CNNs require large amounts of data to avoid overfitting, which is not always available, especially in medical imagery.
