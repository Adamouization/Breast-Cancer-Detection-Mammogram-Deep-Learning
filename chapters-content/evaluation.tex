% \section{Common Pipeline Results}

% All results from the common pipeline were tested on the validation sets to avoid using the test set too early.

% \subsection{Multi-class Classification}

% On the mini-MIAS dataset (multi-class classification problem), an accuracy of 50\% was achieved, with a confusion matrix (see Figure~\ref{fig:evaluation-common-CM-norm_basic-model_mini-MIAS-dataset}) indicating that the CNN is confusing cancerous cases together (benign and malignant), but can tell normal cases apart.

% \begin{figure}[ht]
% \centerline{\includegraphics[width=0.8\textwidth]{figures/evaluation/common/CM-norm_basic-model_mini-MIAS-dataset.png}}
% \caption{\label{fig:evaluation-common-CM-norm_basic-model_mini-MIAS-dataset}Normalised confusion matrix of the classification results after training on the mini-MIAS dataset.}
% \end{figure}

% \subsection{Binary Classification}

% On the CBIS-DDSM dataset (binary classification problem), an accuracy of 65.36\% was initially achieved. Separating the types of mammograms between calcifications and masses revealed that higher accuracies could be achieved. Indeed, an accuracy of 70.36\% was reached when using only mammograms with masses, and 68.2\%  using only mammograms with calcifications (see Figure~\ref{fig:evaluation-cbisddsm-common-mass-vs-calc}).

% \begin{figure}[h]
% \centering
% \begin{subfigure}{.5\textwidth}
%   \centering
%   \includegraphics[width=\textwidth]{figures/evaluation/common/CM-norm_basic-model_CBIS-DDSM-dataset-Calc.png}
%   \caption{Mammograms with calcifications (70.36\%).}
%   \label{fig:evaluation-cbisddsm-common-calc}
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}
%   \centering
%   \includegraphics[width=\textwidth]{figures/evaluation/common/CM-norm_basic-model_CBIS-DDSM-dataset-Mass.png}
%   \caption{Mammograms with masses (68.2\%).}
%   \label{fig:evaluation-cbisddsm-common-mass}
% \end{subfigure}
% \caption{\label{fig:evaluation-cbisddsm-common-mass-vs-calc}Normalised confusion matrices between mammograms with calcifications and masses on the CBIS-DDSM dataset.}
% \end{figure}

% Minor optimisation attempts such as adding additional convolutional layers between the pre-trained VGG19 model and the fully connected layers lowered the accuracy to 55.07\%. Adding more convolutional and spooling layers before the VGG19 pre-trained model slightly increased the accuracy from 65.03\% to 65.36\%, but was much slower to train due to the increased number of trainable parameters that originated from the additional layers.

This section covers the bag-of-tricks approach mentioned in Section~\ref{sec:design-fine-tuning-bagoftricks}, where multiple deep learning techniques covered throughout Chapters~\ref{ch:chapter-litsurvey} \& \ref{ch:chapter-design} are experimented with to determine which improve the performance of the model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model Used}

The model described in Section~\ref{sec:design-cnn-model-decision} is used across all the experiments below, using both VGG19 and MobileNetV2 as the base model (except for Section~\ref{sec:evaluation-cnn-model-experiment}), a fully connected MLP with 512, 32, 2 neurons, a dropout layer using $p=0.2$, the Adam optimiser and resized whole images. Only the dataset, batch size, input size, learning rate (0.001 or 0.0001), class weights, weight initialisation and type of mammograms vary across the experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Base CNN Architectures}
\label{sec:evaluation-cnn-model-experiment}

Five different CNN model architectures (VGG19, ResNet50, InceptionV3, DenseNet121 and MobileNetV2) are tested out as the model's base (pre-trained on ImageNet) using the model described in Section~\ref{sec:design-cnn-model-decision}. For this test, the CBIS-DDSM dataset is used with whole images resized to 512 x 512 pixels, a batch size of 2 and a learning rate of 0.0001.

\input{tables/evaluation/CNN_models}

The results found in Table~\ref{tab:evaluation-cnn-models} clearly reveal that MobileNetV2 unlocks more performance than the others CNN architectures with a higher accuracy and F1 score. The original VGG19 architecture used during the development of the common pipeline is outperformed by more efficient models like DenseNet121 or MobileNetV2, but outperforms the ResNet50 and InceptionV3.\\

However, observing the training and testing runtimes in Figure~\ref{fig:evaluation-CNN_models_experiment-runtimes} reveals that VGG19 takes the longest time to train with 3h50m, whereas the more efficient MobileNetV2 architecture takes 2h46. Additionally, prediction runtime is 2.3 times faster with MobileNetV2 compared to VGG19, which is more useful for clinics as mammogram diagnosis results can be returned faster.

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{figures/evaluation/CNN_models_experiment/runtimes.png}}
\caption{\label{fig:evaluation-CNN_models_experiment-runtimes}Training and prediction runtimes when using different CNN architectures as the base model pre-trained on ImageNet.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Class Weights}

Distinct variations of class weights are used on the CBIS-DDSM dataset to attempt to rectify the negative effects that can be introduced by imbalanced datasets without going through the process of data augmentation, which would considerably slow down the training time by a factor equal to the number of new images generated. Table~\ref{tab:evaluation-class-weights} reports the three class weight values that were tested using the imbalanced CBIS-DDSM dataset with whole images resized to 512 x 512 pixels, a batch size of 2 and a learning rate of 0.0001:
\begin{itemize}
    \item No class weights (dataset remains imbalanced);
    \item Balanced class weights:
    \begin{itemize}
        \item 0.907 for majority class (benign),
        \item 1.113 for minority class (malignant);
    \end{itemize}
    \item +50\% class weight for minority class:
    \begin{itemize}
        \item 1.0 for benign samples,
        \item 1.5 for malignant samples.
    \end{itemize}
\end{itemize}

\input{tables/evaluation/class_weights}

These results clearly depict how including balanced weights to the samples increases the accuracy across different base CNN models by 1.25-1.71\%. However, a manual weight increase for the minority class decreases the accuracy by 0.78-1.1\%, revealing the complexity of finding the right parameters for balancing datasets as the 50\% weight increase for malignant samples made the dataset even more unbalanced. The normalised confusion matrices found in Figures~\ref{fig:evaluation-class_weights_experiment-none} and \ref{fig:evaluation-class_weights_experiment-balanced} expose how including class weights leads to the model being more confused as many malignant samples are classified as benign.

\begin{figure}[h]
\centerline{\includegraphics[width=0.75\textwidth]{figures/evaluation/class_weights_experiment/none.png}}
\caption{\label{fig:evaluation-class_weights_experiment-none}Normalised confusion matrix when no class weights are used with MobileNetV2 as the base model on the CBIS-DDSM dataset.}
\end{figure}

\begin{figure}[h]
\centerline{\includegraphics[width=0.75\textwidth]{figures/evaluation/class_weights_experiment/balanced.png}}
\caption{\label{fig:evaluation-class_weights_experiment-balanced}Normalised confusion matrix when balanced class weights are used with MobileNetV2 as the base model on the CBIS-DDSM dataset.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Input Image Size}

Different image sizes are explored to determine their effect on the model's performance on the CBIS-DDSM dataset. For the smaller image sizes, larger batch sizes are used, whereas for the larger image sizes, smaller batch numbers are defined along with the extra convolutional and spooling layers mentioned in Section~\ref{sec:implementation-sequential-cnn-model} to make use of the larger image size as an attempt to learn lower-level features. The following image sizes are used:
\begin{itemize}
    \item 224 x 224 pixels (chosen as most CNNs pre-trained on ImageNet use this size) with batch size of 8;
    \item 512 x 512 pixels with batch size of 2;
    \item 1024 x 1024 pixels (with additional convolutional/spooling layers) with batch size of 2.
\end{itemize}

\input{tables/evaluation/image_size}

The results in Table~\ref{tab:evaluation-image-size} clearly expose the accuracy increase by using 512 pixels-wide input size rather than 224, with a 0.63\% increase on VGG19 and 4.52\% increase on MobileNetV2. However, further increasing the input size to 1024 pixels has no positive effect as the accuracy drops by 4.52\% on VGG19 and leads to an Out Of Memory (OOM) error on MobileNetV2 despite lowering the batch size to 1.\\

\begin{figure}[H]
\centerline{\includegraphics[width=1.2\textwidth]{figures/evaluation/image_size_experiment/training_summary.png}}
\caption{\label{fig:evaluation-image_size_experiment-training_summary}Evolution of the accuracy and loss during both training phases when testing 1024x1024 input size on VGG19.}
\end{figure}

Observing the evolution of the training accuracy and loss when using 1024 x 1024 pixels input size on VGG19 (see Figure~\ref{fig:evaluation-image_size_experiment-training_summary}), it can be seen that the validation loss increases while the training loss decreases and that both sets' training accuracies are increasing as well; which is a typical pattern of a model overfitting the data. Because the model is overfitting the data, a very high precision (66.94\%) but low recall (59.28\%) is witnessed in Table~\ref{tab:evaluation-image-size} for 1024x1024 input size, which is extremely bad as a BCD system that detects malignant cases as benign could lead to the death of the patient.\\

As expected, increasing the image size also increases the training runtime (see Figure~\ref{fig:evaluation-image_size_experiment-runtimes}), which is boosted by a factor of 2.4 when increasing from 224 to 512 pixels, and a factor of 2.8 from 512 to 1024 pixels on VGG19. However, another advantage of MobileNetV2 over VGG19 is that it scales better to larger input sizes as increasing the input from 224 to 512 pixels only raises the runtime by a factor of 1.54, and prediction times are quicker than VGG19 predictions (13.5 minutes on average for MobileNetV2 compared to 21.3 minutes for VGG19).

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{figures/evaluation/image_size_experiment/runtimes.png}}
\caption{\label{fig:evaluation-image_size_experiment-runtimes}Training and prediction runtimes when using different input image sizes.}
\end{figure}

Nevertheless, the accuracy/ training runtime trade-off is not primordial in breast cancer detection as the goal is to develop a system that can correctly diagnose early forms of cancers in mammograms as accurately as possible, regardless of the runtime. Ultimately, prediction runtimes will matter when used in clinics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Varying Amounts of Transfer Learning}
\label{sec:evaluation-transfer-learning}

This experiment consists of expanding upon the concept of transfer learning. Instead of using a CNN pre-trained on ImageNet, weights of a model trained on a \textit{binarised} mini-MIAS dataset are transferred to the CBIS-DDSM dataset. To binarise the mini-MIAS dataset, the normal cases are dropped altogether, resulting in a very small dataset of 115 abnormal images (64 benign and 51 malignant). The model is then trained on the binary mini-MIAS dataset and its final weights are saved. Four different experiments using an identical CNN architecture are tested to assess the effect of transfer learning from the binarised mini-MIAS dataset to the larger CBIS-DDSM dataset:
\begin{itemize}
    \item Transfer learning of all layer weights (Base CNN and fully connected layers instantiated with binary mini-MIAS weights);
    \item Transfer learning of fully connected layer weights (fully connected layers instantiated with binary mini-MIAS weights, base CNN layers instantiated with ImageNet weights);
    \item Transfer learning of ImageNet weights only (fully connected layers instantiated with random weights, base CNN layers instantiated with ImageNet weights);
    \item No transfer learning (Base CNN and fully connected layers instantiated with random weights).
\end{itemize}

\input{tables/evaluation/transfer_learning}

The results clearly indicate that the closer the weight initialisation were to the binary mini-MIAS dataset, the lower the test accuracy was, clearly indicating that the model did not generalise well to CBIS-DDSM mammograms. This could be linked to the advantages of random weight initialisations that give a better chance to the model to explore new combinations of weights rather than immediately converging towards a known solution. Indeed, Figure~\ref{fig:evaluation-individual-TL-training} shows how quickly the loss drops when using binary mini-MIAS weights, whereas the model slowly converges towards a lower loss when using random weight initialisations (no transfer learning).

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/evaluation/individual/TL-none.png}
  \caption{No transfer learning.}
  \label{fig:evaluation-individual-TL-none}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/evaluation/individual/TL-all.png}
  \caption{Binary mini-MIAS on all layers.}
  \label{fig:evaluation-individual-TL-all}
\end{subfigure}
\caption{\label{fig:evaluation-individual-TL-training}Evolution of the training accuracy and the training/validation losses across epochs with a learning rate of 0.001.}
\end{figure}

Additionally, it is worth noting that the training runtimes are similar across all tests, as it is the early stopping conditions (validation loss not decreasing) that dictates the stopping conditions.

% \section{Individual Optimisation Results}

% Using more advanced models than VGG19, which is a sequential CNN, such as InceptionV3, yielded poor results and required much longer training times. Indeed, the evolution of the validation loss visualised in Figure~\ref{fig:evaluation-individual-inceptionv3-poor-training}  confirms that  the InceptionV3 model highly overfits the data, suggested an urgent need for regularisation techniques such as Dropout.\\

% \begin{figure}[ht]
% \centerline{\includegraphics[width=0.8\textwidth]{figures/evaluation/individual/InceptionV3-poor-training.png}}
% \caption{\label{fig:evaluation-individual-inceptionv3-poor-training}Evolution of validation loss during training of the InceptionV3 model on the CBIS-DDSM dataset.}
% \end{figure}

% Potential future comparisons:
% \begin{itemize}
%     \item Different CNN models (VGG19, ResNet50V2, InceptionV3, Xception)
%     \item Using transfer learning (pre-trained model on ImageNet)
%     \item Using different types of mammograms (calcifications only, masses only, both)
% \end{itemize}

%%%%%%%%

\section{Mammogram Types}

To assess how the model would react to being fed only specific samples of a single mammogram type, the CBIS-DDSM dataset was separated into only masses samples and only calcifications samples. Three different experiments using identical an CNN architecture were tested:
\begin{itemize}
    \item All types of mammograms (masses + calcifications);
    \item Mass mammograms only;
    \item Calcification mammograms only.
\end{itemize}

\input{tables/evaluation/mammogram_types}

\begin{figure}[ht]
\centerline{\includegraphics[width=\textwidth]{figures/evaluation/mammogram_type_experiment/runtimes.png}}
\caption{\label{fig:evaluation-mammogram_type_experiment-runtimes.png}Training and prediction runtimes when using different mammogram types.}
\end{figure}

These results show that the model learns the data much better when masses and calcifications are separated, reaching 64.35\% and 66.67\% accuracy respectively on the test set, but only managing 59.44\% when using the full CBIS-DDSM dataset. Indeed, the normalised confusion matrix indicates that all instances are classified as ``benign'' using the full dataset containing both types, which could indicate that the model gets confused when dealing with multiple views.\\

Harder to detect masses than calcifications \citep{Elter2009}.
