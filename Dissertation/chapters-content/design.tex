\section{General Design Decisions}

\subsection{Programming Language}

Choosing a programming language is one of the most essential aspect to take into account as it is the medium used to transform the system from design to implementation. However, choosing from over 250 different programming languages \citep{tiobe} can be tricky, which is why multiple views have to be evaluated before choosing a programming language. Four popular programming languages are considered for this project: Python, Java, R and  Javascript.\\

The first element to consider is the availability of third-party libraries used for implementing common machine learning methods, as well as data pre-processing, manipulation and visualisation techniques found in deep learning systems to avoid manually implementing them. The most popular libraries nowadays consist of Tensorflow (which is available in Python, R and Javascript) and PyTorch (which is available in Python). Additionally, Compute Unified Device Architecture (CUDA) support for GPU optimisations, CNN support and pre-trained models need to be included for the implementation of the desired deep learning breast cancer detection system, which are available in all libraries.\\

In terms of speed, compiled languages are quicker than interpreted languages. However, the main bottleneck in a deep learning system is the training phase of the model, which mainly relies on the library used rather than the language itself. Therefore, speed is not taken into account. Finally, of all the programming languages mentioned, Python is the favoured one in terms of personal preference, familiarity, and experience, especially when applied to machine learning implementations. Therefore, the wide support for machine learning libraries in Python, coupled with the personal preference for the language, make Python the obvious programming language candidate for this project.\\

For a complete review of the main pros and cons considered between when deciding between Python, Java, R and Javascript, refer to Appendix~\ref{sec:appendix-programming-languages-comparison}.

\subsection{Deep Learning Framework}

The two most popular deep learning frameworks nowadays are Tensorflow coupled with Keras, and PyTorch. Tensorflow/Keras being relatively older than PyTorch, have got more online support, which is confirmed by the number of daily downloads Keras has compared to PyTorch (ten times more), as well as the number of mentions in academic papers (see figures  in Appendix~\ref{sec:appendix-keras_vs_pytorch})

\subsection{Interface}

A Command-Line Interface (CLI) is selected, allowing arguments and flags to be passed to execute different sections of the code. Arguments control the dataset to use, the CNN model, and the mode to run in (training or testing). Flags control the verbose mode to print more statements in the terminal for debugging purposed. The full set of instructions to run the code can be found in Appendix~\ref{ch:appendix-usage-instructions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Deep Learning Pipeline Design Considerations}

The deep learning pipeline implemented for the task of breast cancer detection can be broken down in four distinct phases condensed in Figure~\ref{fig:design-flowchart}:

\begin{itemize}
    \item Data pre-processing: loading the datasets in memory and encoding them for classification.
    \item Model training: creating a CNN model and fitting the training data.
    \item Result visualisation: plotting the predictions.
    \item Hyperparameter fine-tuning: running grid search with different combinations of hyperparameters to find the best combination.
\end{itemize}

\begin{figure}[ht]
\centerline{\includegraphics[width=1.1\textwidth]{Dissertation/figures/design/design flowchart.png}}
\caption{\label{fig:design-flowchart}High-level flowchart of the breast cancer detection deep learning pipeline. Created using draw.io.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%

\subsection{Data pre-processing}

\subsubsection{Dataset \& classification type}

An early design decision taken as a group consisted in which datasets to use. The small size of mini-MIAS dataset makes it useful for preliminary testing. Additionally, it contains three classes (normal, benign and malignant cases), rendering it more useful from a clinical point of view.\\  

The CBIS-DDSM dataset was chosen over the DDSM as it is more recent than the DDSM dataset, using uncompressed images in DICOM format rather than LJPEG format, which is deprecated nowadays. The large uncompressed format means that the mammograms can be fed into the CNN with larger sizes, allowing the model to learn more low-level features.

\subsubsection{Dataset balance}

As this is a classification task, it is essential to visualise the distribution of classes in the datasets to determine whether the data is skewed or not (whether some classes are much more frequent than other classes \citep{Geron2019}).  The class distributions are plotted in Figure~\ref{fig:design-datasets-balance}.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.94\textwidth]{Dissertation/figures/design/mini-mias-balance.png}
  \caption{mini-MIAS class distribution.}
  \label{fig:design-mini-mias-balance}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Dissertation/figures/design/cbis-ddsm-balance.png}
  \caption{CBIS-DDSM class distribution.}
  \label{fig:cbis-ddsm-balance}
\end{subfigure}
\caption{\label{fig:design-datasets-balance}Class distribution for the mini-MIAS and the CBIS-DDSM datasets. Histograms generated in Excel.}
\end{figure}

These bar charts reveal that the mini-MIAS dataset is heavily unbalanced, which must be taken into account when analysing the classifiers' scores. Indeed, using an  evaluation metric such as accuracy would be misleading as it would not be representative of how well the classifier fitted the data. For instance, if a dumb classifier that always classified an image as ``normal'' was created, it would achieve 64.28\% accuracy on the mini-MIAS dataset. Therefore, other metrics such as confusion matrices, precision, recall and F1 scores could be used, which are further explored in Section~\ref{sec:design-results-visualisation}). A potential solution to counter the imbalance would be to oversample the dataset, which can be achieved via data augmentation.

\subsubsection{Data loading}

The mini-MIAS dataset is very small in size (339 Mb before pre-processing, 202 Mb after pre-processing), containing only 322 images. It can therefore be loaded into memory without any data loading optimisation techniques. However, the CBIS-DDSM dataset is much larger, containing 10,239 images that cover 163.6 Gb of disk space. The dataset therefore cannot be loaded in memory in a single import and needs to be loaded in batche to be fed into the CNN sequentially.\\ % mention batches and caching

As the labels for each mammogram are in categorical string format, they must be encoded into a numerical format. One-hot encoding is therefore chosen as it suits the sparse representation of the data, which is made up of only two or three target categories, depending on the dataset being used. The one-hot encodings of the labels can be seen in Table~\ref{tab:label-encoding-example}.

\input{Dissertation/tables/label-conversion-example}

%%%%%%%%%%%%%%%%%%%%

\subsection{Model training}

To complete:
\begin{itemize}
    \item Training/testing/validation split using stratified and shuffling splits (keep class balance and reorder images in directories).
    \item CNN models with pre-trained weights on ImageNet to avoid training from scratch. However, this  forces the input images to have the same size as the images in ImageNet it was trained on.
    \item Early stopping conditions.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%

\subsection{Result visualisation}
\label{sec:design-results-visualisation}

Different output metrics are chosen to assess how well the CNN learned the mammograms data and generalises to unseen cases. A combination of numerical metrics and visual metrics are used:

\begin{itemize}
    \item Numerical metrics:
    \begin{itemize}
        \item Overall accuracy
        \item Precision
        \item Recall
        \item F1 score
    \end{itemize}
    \item Visual metrics (plots):
    \begin{itemize}
        \item Evolution of training/validation accuracies and losses over the number of epochs
        \item Confusion matrices
        \begin{itemize}
            \item Classification counts
            \item Normalised (values between 0 and 1)
        \end{itemize}
        \item Receiver Operating Characteristic (ROC) curve
    \end{itemize}
\end{itemize}
